{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab1301c",
   "metadata": {},
   "source": [
    "# Databricks\n",
    "\n",
    "## Comprehensive Practice Notebook\n",
    "\n",
    "This notebook provides hands-on practice for common Databricks. Each section includes examples that demonstrate key concepts and best practices.\n",
    "\n",
    "### What's Covered:\n",
    "1. **Environment Setup** - Spark configuration and initialization\n",
    "2. **Data Ingestion** - Multiple data sources and Delta Lake integration\n",
    "3. **Data Transformations** - PySpark operations and SQL\n",
    "4. **Streaming Data** - Real-time processing patterns\n",
    "5. **Data Quality** - Validation and constraints\n",
    "6. **Performance Optimization** - Query tuning and optimization\n",
    "7. **MLflow Integration** - ML lifecycle management\n",
    "8. **Unity Catalog** - Data governance and security\n",
    "9. **Job Workflows** - Scheduling and monitoring\n",
    "10. **Debugging & Monitoring** - Troubleshooting techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289f6b3",
   "metadata": {},
   "source": [
    "## 1. Setup Databricks Environment\n",
    "\n",
    "### Initialize Spark Session and Configure Cluster Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a82a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data engineering\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "# Initialize Spark Session (in Databricks, this is already available as 'spark')\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Python Version: {spark.sparkContext.pythonVer}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "# Configure Spark settings for optimal performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "print(\"Spark session configured successfully!\")\n",
    "print(\"Key configurations:\")\n",
    "for key in [\"spark.sql.adaptive.enabled\", \"spark.databricks.delta.optimizeWrite.enabled\"]:\n",
    "    print(f\"  {key}: {spark.conf.get(key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "\n",
    "# Create sample customer data\n",
    "customers_data = [\n",
    "    (1, \"Alice Johnson\", \"alice@email.com\", \"2023-01-15\", \"Premium\", \"New York\"),\n",
    "    (2, \"Bob Smith\", \"bob@email.com\", \"2023-02-20\", \"Standard\", \"California\"), \n",
    "    (3, \"Carol Davis\", \"carol@email.com\", \"2023-03-10\", \"Premium\", \"Texas\"),\n",
    "    (4, \"David Wilson\", \"david@email.com\", \"2023-04-05\", \"Basic\", \"Florida\"),\n",
    "    (5, \"Eve Brown\", \"eve@email.com\", \"2023-05-12\", \"Standard\", \"New York\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"registration_date\", StringType(), False),\n",
    "    StructField(\"tier\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, customers_schema)\n",
    "\n",
    "# Create sample orders data\n",
    "orders_data = [\n",
    "    (101, 1, \"2023-06-01\", 150.00, \"Electronics\"),\n",
    "    (102, 2, \"2023-06-02\", 89.50, \"Books\"),\n",
    "    (103, 1, \"2023-06-03\", 299.99, \"Electronics\"),\n",
    "    (104, 3, \"2023-06-04\", 45.00, \"Clothing\"),\n",
    "    (105, 4, \"2023-06-05\", 199.99, \"Electronics\"),\n",
    "    (106, 2, \"2023-06-06\", 25.99, \"Books\"),\n",
    "    (107, 5, \"2023-06-07\", 75.50, \"Clothing\"),\n",
    "    (108, 1, \"2023-06-08\", 399.00, \"Electronics\")\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"category\", StringType(), False)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema)\n",
    "\n",
    "print(\"Sample datasets created:\")\n",
    "print(f\"Customers: {customers_df.count()} records\")\n",
    "print(f\"Orders: {orders_df.count()} records\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample Customer Data:\")\n",
    "customers_df.show(3, truncate=False)\n",
    "\n",
    "print(\"\\n Sample Orders Data:\")\n",
    "orders_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdbc45",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion with Delta Lake\n",
    "\n",
    "### Demonstrate reading from various data sources and writing to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32194631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta Lake tables\n",
    "\n",
    "# Define base paths for our Delta tables\n",
    "base_path = \"/tmp/delta-practice\"\n",
    "customers_table_path = f\"{base_path}/customers\"\n",
    "orders_table_path = f\"{base_path}/orders\"\n",
    "\n",
    "# Write DataFrames to Delta Lake format\n",
    "print(\"Writing data to Delta Lake...\")\n",
    "\n",
    "# Method 1: Write to Delta Lake using DataFrame API\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(customers_table_path)\n",
    "\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .save(orders_table_path)\n",
    "\n",
    "print(\"Delta tables created successfully!\")\n",
    "\n",
    "# Method 2: Create managed tables\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"customers_managed\")\n",
    "\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .saveAsTable(\"orders_managed\")\n",
    "\n",
    "print(\"Managed Delta tables created!\")\n",
    "\n",
    "# Verify tables exist\n",
    "print(\"\\n Available tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Read data back from Delta Lake\n",
    "print(\"\\n Reading from Delta Lake:\")\n",
    "delta_customers = spark.read.format(\"delta\").load(customers_table_path)\n",
    "delta_orders = spark.read.format(\"delta\").load(orders_table_path)\n",
    "\n",
    "print(f\"Customers from Delta: {delta_customers.count()} records\")\n",
    "print(f\"Orders from Delta: {delta_orders.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f12f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Lake CRUD Operations\n",
    "\n",
    "# CREATE - Insert new records\n",
    "print(\"CRUD Operations with Delta Lake\\n\")\n",
    "\n",
    "# Insert new customer\n",
    "new_customer_data = [(6, \"Frank Miller\", \"frank@email.com\", \"2023-06-10\", \"Premium\", \"Washington\")]\n",
    "new_customer_df = spark.createDataFrame(new_customer_data, customers_schema)\n",
    "\n",
    "new_customer_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(customers_table_path)\n",
    "\n",
    "print(\"INSERT: Added new customer\")\n",
    "\n",
    "# READ - Query with filters\n",
    "premium_customers = spark.read.format(\"delta\").load(customers_table_path) \\\n",
    "    .filter(col(\"tier\") == \"Premium\")\n",
    "\n",
    "print(f\"Premium customers: {premium_customers.count()}\")\n",
    "premium_customers.select(\"name\", \"tier\", \"state\").show()\n",
    "\n",
    "# UPDATE - Using Delta Lake merge operation\n",
    "print(\"\\n UPDATE operations:\")\n",
    "\n",
    "# Create update data (customer tier change)\n",
    "update_data = [(2, \"Bob Smith\", \"bob@email.com\", \"2023-02-20\", \"Premium\", \"California\")]\n",
    "updates_df = spark.createDataFrame(update_data, customers_schema)\n",
    "\n",
    "# Perform merge (upsert)\n",
    "delta_customers_table = DeltaTable.forPath(spark, customers_table_path)\n",
    "\n",
    "delta_customers_table.alias(\"customers\") \\\n",
    "    .merge(\n",
    "        updates_df.alias(\"updates\"),\n",
    "        \"customers.customer_id = updates.customer_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "print(\"UPDATE: Customer tier updated using MERGE\")\n",
    "\n",
    "# DELETE - Remove records\n",
    "print(\"\\n  DELETE operations:\")\n",
    "\n",
    "# Delete using Delta table API\n",
    "delta_customers_table.delete(col(\"customer_id\") == 6)\n",
    "print(\"DELETE: Removed customer using Delta API\")\n",
    "\n",
    "# Verify final state\n",
    "print(f\"\\nFinal customer count: {spark.read.format('delta').load(customers_table_path).count()}\")\n",
    "\n",
    "# Show version history (time travel capability)\n",
    "print(\"\\n Delta Lake Version History:\")\n",
    "delta_customers_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15438401",
   "metadata": {},
   "source": [
    "## 3. Data Transformation with PySpark\n",
    "\n",
    "### Common transformation patterns using DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common DataFrame transformations\n",
    "\n",
    "# Read our Delta tables\n",
    "customers = spark.read.format(\"delta\").load(customers_table_path)\n",
    "orders = spark.read.format(\"delta\").load(orders_table_path)\n",
    "\n",
    "print(\"DataFrame Transformations\\n\")\n",
    "\n",
    "# 1. Basic transformations\n",
    "print(\"1. Basic Transformations:\")\n",
    "\n",
    "# Add computed columns\n",
    "customers_enhanced = customers \\\n",
    "    .withColumn(\"registration_year\", year(to_date(col(\"registration_date\")))) \\\n",
    "    .withColumn(\"email_domain\", split(col(\"email\"), \"@\")[1]) \\\n",
    "    .withColumn(\"name_length\", length(col(\"name\")))\n",
    "\n",
    "customers_enhanced.select(\"name\", \"registration_year\", \"email_domain\", \"name_length\").show()\n",
    "\n",
    "# 2. Aggregations\n",
    "print(\"\\n 2. Aggregation Operations:\")\n",
    "\n",
    "# Customer order summary\n",
    "customer_summary = orders.groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_spent\"),\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        avg(\"amount\").alias(\"avg_order_value\"),\n",
    "        max(\"amount\").alias(\"max_order\"),\n",
    "        collect_list(\"category\").alias(\"categories_purchased\")\n",
    "    )\n",
    "\n",
    "customer_summary.show(truncate=False)\n",
    "\n",
    "# 3. Window functions - advanced topic often asked\n",
    "print(\"\\n 3. Window Functions:\")\n",
    "\n",
    "# Rank customers by total spending\n",
    "window_spec = Window.orderBy(desc(\"total_spent\"))\n",
    "\n",
    "customer_ranking = customer_summary \\\n",
    "    .withColumn(\"spending_rank\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"spending_percentile\", percent_rank().over(window_spec))\n",
    "\n",
    "customer_ranking.select(\"customer_id\", \"total_spent\", \"spending_rank\", \"spending_percentile\").show()\n",
    "\n",
    "# Running totals by customer\n",
    "order_window = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "orders_with_running_total = orders \\\n",
    "    .withColumn(\"running_total\", sum(\"amount\").over(order_window)) \\\n",
    "    .withColumn(\"order_number\", row_number().over(order_window))\n",
    "\n",
    "print(\"\\n Running totals by customer:\")\n",
    "orders_with_running_total.select(\"customer_id\", \"order_date\", \"amount\", \"running_total\", \"order_number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins and Complex Transformations\n",
    "\n",
    "print(\"Join Operations and Complex Transformations\\n\")\n",
    "\n",
    "# 1. Different types of joins\n",
    "print(\"1. Join Types:\")\n",
    "\n",
    "# Inner join - most common\n",
    "customer_orders = customers.alias(\"c\").join(\n",
    "    orders.alias(\"o\"), \n",
    "    col(\"c.customer_id\") == col(\"o.customer_id\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"c.name\"),\n",
    "    col(\"c.tier\"), \n",
    "    col(\"c.state\"),\n",
    "    col(\"o.order_date\"),\n",
    "    col(\"o.amount\"),\n",
    "    col(\"o.category\")\n",
    ")\n",
    "\n",
    "print(\"Inner Join - Customers with their orders:\")\n",
    "customer_orders.show(5)\n",
    "\n",
    "# Left join to find customers without orders\n",
    "customers_with_orders = customers.alias(\"c\").join(\n",
    "    orders.alias(\"o\"),\n",
    "    col(\"c.customer_id\") == col(\"o.customer_id\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"c.customer_id\"),\n",
    "    col(\"c.name\"),\n",
    "    col(\"o.order_id\").isNull().alias(\"has_no_orders\")\n",
    ")\n",
    "\n",
    "customers_without_orders = customers_with_orders.filter(col(\"has_no_orders\") == True)\n",
    "print(f\"\\nCustomers without orders: {customers_without_orders.count()}\")\n",
    "\n",
    "# 2. User Defined Functions (UDFs)\n",
    "print(\"\\n 2. User Defined Functions:\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define UDF to categorize spending\n",
    "def spending_category(total_spent):\n",
    "    if total_spent > 500:\n",
    "        return \"High Spender\"\n",
    "    elif total_spent > 200:\n",
    "        return \"Medium Spender\"\n",
    "    else:\n",
    "        return \"Low Spender\"\n",
    "\n",
    "# Register UDF\n",
    "spending_category_udf = udf(spending_category, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "customers_categorized = customer_summary \\\n",
    "    .withColumn(\"spending_category\", spending_category_udf(col(\"total_spent\")))\n",
    "\n",
    "print(\"Customer spending categories:\")\n",
    "customers_categorized.select(\"customer_id\", \"total_spent\", \"spending_category\").show()\n",
    "\n",
    "# 3. Pivot operations - advanced transformation\n",
    "print(\"\\n 3. Pivot Operations:\")\n",
    "\n",
    "# Pivot to see spending by category per customer\n",
    "spending_by_category = orders.groupBy(\"customer_id\") \\\n",
    "    .pivot(\"category\") \\\n",
    "    .sum(\"amount\") \\\n",
    "    .fillna(0)\n",
    "\n",
    "print(\"Spending by category (pivoted):\")\n",
    "spending_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27d168",
   "metadata": {},
   "source": [
    "## 4. Working with Streaming Data\n",
    "\n",
    "### Real-time data processing patterns and structured streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02086b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Data Processing\n",
    "\n",
    "print(\"Structured Streaming Examples\\n\")\n",
    "\n",
    "# Create a streaming source (simulating real-time data)\n",
    "# In real scenarios, this would be Kafka, Event Hubs, etc.\n",
    "\n",
    "# Generate sample streaming data\n",
    "import time\n",
    "import random\n",
    "\n",
    "def generate_streaming_orders():\n",
    "    \"\"\"Generate sample orders for streaming demo\"\"\"\n",
    "    customers_list = [1, 2, 3, 4, 5]\n",
    "    categories = [\"Electronics\", \"Books\", \"Clothing\", \"Sports\"]\n",
    "    \n",
    "    streaming_orders = []\n",
    "    for i in range(20):\n",
    "        order = {\n",
    "            \"order_id\": 1000 + i,\n",
    "            \"customer_id\": random.choice(customers_list),\n",
    "            \"timestamp\": datetime.now() + timedelta(seconds=i),\n",
    "            \"amount\": round(random.uniform(10, 500), 2),\n",
    "            \"category\": random.choice(categories)\n",
    "        }\n",
    "        streaming_orders.append(order)\n",
    "    return streaming_orders\n",
    "\n",
    "# Create streaming DataFrame\n",
    "streaming_orders = generate_streaming_orders()\n",
    "streaming_df = spark.createDataFrame(streaming_orders)\n",
    "\n",
    "# Save to Delta for streaming source\n",
    "streaming_source_path = f\"{base_path}/streaming_orders_source\"\n",
    "streaming_df.write.format(\"delta\").mode(\"overwrite\").save(streaming_source_path)\n",
    "\n",
    "print(\"Sample streaming orders created:\")\n",
    "streaming_df.show(5)\n",
    "\n",
    "# Set up streaming read\n",
    "print(\"\\n Setting up streaming pipeline...\")\n",
    "\n",
    "# Read stream from Delta table (Auto Loader pattern)\n",
    "streaming_orders_read = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(streaming_source_path)\n",
    "\n",
    "print(\"Streaming source configured\")\n",
    "\n",
    "# Stream processing with aggregations\n",
    "print(\"\\n Real-time aggregations:\")\n",
    "\n",
    "# Aggregate by sliding window\n",
    "streaming_aggregated = streaming_orders_read \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\", \"1 minute\"),\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        count(\"order_id\").alias(\"order_count\"),\n",
    "        avg(\"amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "# Display schema for streaming aggregation\n",
    "print(\"Streaming aggregation schema:\")\n",
    "streaming_aggregated.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming patterns\n",
    "\n",
    "print(\"Advanced Streaming Patterns\\n\")\n",
    "\n",
    "# 1. Streaming Upserts (Merge)\n",
    "print(\"1. Streaming Upserts with Delta Lake:\")\n",
    "\n",
    "def streaming_upsert_demo():\n",
    "    \"\"\"Demonstrate streaming upsert pattern\"\"\"\n",
    "    \n",
    "    # Create target table for upserts\n",
    "    upsert_target_path = f\"{base_path}/customer_metrics\"\n",
    "    \n",
    "    # Initialize with some data\n",
    "    initial_metrics = [\n",
    "        (1, 150.0, 1, datetime.now()),\n",
    "        (2, 89.5, 1, datetime.now()),\n",
    "        (3, 45.0, 1, datetime.now())\n",
    "    ]\n",
    "    \n",
    "    initial_schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), False),\n",
    "        StructField(\"total_spent\", DoubleType(), False),\n",
    "        StructField(\"order_count\", IntegerType(), False),\n",
    "        StructField(\"last_updated\", TimestampType(), False)\n",
    "    ])\n",
    "    \n",
    "    initial_df = spark.createDataFrame(initial_metrics, initial_schema)\n",
    "    initial_df.write.format(\"delta\").mode(\"overwrite\").save(upsert_target_path)\n",
    "    \n",
    "    print(\"Target table initialized\")\n",
    "    \n",
    "    # Show upsert function (would be used in foreachBatch)\n",
    "    def upsert_function(micro_batch_df, batch_id):\n",
    "        \"\"\"Function to perform upserts in streaming\"\"\"\n",
    "        \n",
    "        # Aggregate incoming batch by customer\n",
    "        batch_aggregated = micro_batch_df.groupBy(\"customer_id\").agg(\n",
    "            sum(\"amount\").alias(\"batch_total\"),\n",
    "            count(\"order_id\").alias(\"batch_orders\")\n",
    "        ).withColumn(\"last_updated\", current_timestamp())\n",
    "        \n",
    "        # Perform merge (upsert)\n",
    "        target_table = DeltaTable.forPath(spark, upsert_target_path)\n",
    "        \n",
    "        target_table.alias(\"target\").merge(\n",
    "            batch_aggregated.alias(\"updates\"),\n",
    "            \"target.customer_id = updates.customer_id\"\n",
    "        ).whenMatchedUpdate(set = {\n",
    "            \"total_spent\": \"target.total_spent + updates.batch_total\",\n",
    "            \"order_count\": \"target.order_count + updates.batch_orders\", \n",
    "            \"last_updated\": \"updates.last_updated\"\n",
    "        }).whenNotMatchedInsert(values = {\n",
    "            \"customer_id\": \"updates.customer_id\",\n",
    "            \"total_spent\": \"updates.batch_total\",\n",
    "            \"order_count\": \"updates.batch_orders\",\n",
    "            \"last_updated\": \"updates.last_updated\"\n",
    "        }).execute()\n",
    "        \n",
    "        print(f\"Batch {batch_id}: Upsert completed\")\n",
    "    \n",
    "    return upsert_function\n",
    "\n",
    "upsert_func = streaming_upsert_demo()\n",
    "\n",
    "# 2. Watermarking for late data handling\n",
    "print(\"\\n 2. Watermarking Strategy:\")\n",
    "\n",
    "# Example of watermark configuration\n",
    "watermarked_stream = streaming_orders_read \\\n",
    "    .withWatermark(\"timestamp\", \"15 minutes\")  # Allow 15 minutes for late data\n",
    "\n",
    "print(\"Watermark configured for late data handling\")\n",
    "\n",
    "# 3. Output modes explanation\n",
    "print(\"\\n 3. Streaming Output Modes:\")\n",
    "output_modes = {\n",
    "    \"append\": \"Only new rows added to result table\",\n",
    "    \"update\": \"Updated rows in result table\", \n",
    "    \"complete\": \"Entire result table rewritten\"\n",
    "}\n",
    "\n",
    "for mode, description in output_modes.items():\n",
    "    print(f\"  ‚Ä¢ {mode}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04490f8c",
   "metadata": {},
   "source": [
    "## 5. Data Quality and Validation\n",
    "\n",
    "### Implement data quality checks and constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Framework - Critical for production systems\n",
    "\n",
    "print(\"üîç Data Quality and Validation\\n\")\n",
    "\n",
    "# 1. Basic data quality checks\n",
    "print(\"1. Basic Data Quality Checks:\")\n",
    "\n",
    "def perform_data_quality_checks(df, table_name):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    total_rows = df.count()\n",
    "    \n",
    "    quality_report = {\n",
    "        \"table_name\": table_name,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"checks\": []\n",
    "    }\n",
    "    \n",
    "    # Null value checks\n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        null_percentage = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        \n",
    "        quality_report[\"checks\"].append({\n",
    "            \"check_type\": \"null_check\",\n",
    "            \"column\": column,\n",
    "            \"null_count\": null_count,\n",
    "            \"null_percentage\": round(null_percentage, 2),\n",
    "            \"status\": \"PASS\" if null_percentage < 5 else \"FAIL\"\n",
    "        })\n",
    "    \n",
    "    # Duplicate checks\n",
    "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "    quality_report[\"duplicate_rows\"] = duplicate_count\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Run quality checks on our data\n",
    "customers_quality = perform_data_quality_checks(customers, \"customers\")\n",
    "orders_quality = perform_data_quality_checks(orders, \"orders\")\n",
    "\n",
    "print(\"Customer Data Quality Report:\")\n",
    "for check in customers_quality[\"checks\"][:3]:  # Show first 3 checks\n",
    "    print(f\"  {check['column']}: {check['null_count']} nulls ({check['null_percentage']}%) - {check['status']}\")\n",
    "\n",
    "print(f\"\\nDuplicate rows: {customers_quality['duplicate_rows']}\")\n",
    "\n",
    "# 2. Custom validation rules\n",
    "print(\"\\n 2. Custom Validation Rules:\")\n",
    "\n",
    "def validate_email_format(df):\n",
    "    \"\"\"Validate email format\"\"\"\n",
    "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    \n",
    "    invalid_emails = df.filter(~col(\"email\").rlike(email_pattern))\n",
    "    invalid_count = invalid_emails.count()\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        print(f\"Found {invalid_count} invalid email addresses:\")\n",
    "        invalid_emails.select(\"customer_id\", \"name\", \"email\").show()\n",
    "    else:\n",
    "        print(\"All emails have valid format\")\n",
    "    \n",
    "    return invalid_count == 0\n",
    "\n",
    "def validate_business_rules(df):\n",
    "    \"\"\"Validate business logic rules\"\"\"\n",
    "    \n",
    "    print(\"üîç Business Rule Validations:\")\n",
    "    \n",
    "    # Rule 1: All orders must have positive amounts\n",
    "    negative_amounts = df.filter(col(\"amount\") <= 0).count()\n",
    "    print(f\"  Negative amounts: {negative_amounts} (should be 0)\")\n",
    "    \n",
    "    # Rule 2: Order dates should not be in the future\n",
    "    future_dates = df.filter(col(\"order_date\") > current_date()).count()\n",
    "    print(f\"  Future dates: {future_dates} (should be 0)\")\n",
    "    \n",
    "    # Rule 3: Customer IDs must exist in customers table\n",
    "    customer_ids = customers.select(\"customer_id\").distinct()\n",
    "    invalid_customers = df.join(customer_ids, \"customer_id\", \"left_anti\").count()\n",
    "    print(f\"  Invalid customer IDs: {invalid_customers} (should be 0)\")\n",
    "    \n",
    "    return negative_amounts == 0 and future_dates == 0 and invalid_customers == 0\n",
    "\n",
    "# Run validations\n",
    "email_valid = validate_email_format(customers)\n",
    "business_rules_valid = validate_business_rules(orders)\n",
    "\n",
    "print(f\"\\n Overall data quality: {'PASS' if email_valid and business_rules_valid else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96145398",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Techniques\n",
    "\n",
    "### Query optimization, caching, and performance tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimization\n",
    "\n",
    "print(\"Performance Optimization Techniques\\n\")\n",
    "\n",
    "# 1. Caching strategies\n",
    "print(\"1. Caching Strategies:\")\n",
    "\n",
    "# Cache frequently accessed DataFrames\n",
    "customers.cache()\n",
    "orders.cache()\n",
    "\n",
    "print(\"DataFrames cached in memory\")\n",
    "\n",
    "# Check what's cached\n",
    "cached_tables = spark.sql(\"SHOW TABLES\").filter(col(\"isTemporary\") == True)\n",
    "print(\"Cached tables:\")\n",
    "cached_tables.show()\n",
    "\n",
    "# 2. Broadcast joins for small tables\n",
    "print(\"\\n 2. Broadcast Joins:\")\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Example: Broadcasting smaller customer table\n",
    "large_orders_df = orders.union(orders).union(orders)  # Simulate larger dataset\n",
    "print(f\"Large orders dataset: {large_orders_df.count()} records\")\n",
    "\n",
    "# Regular join vs broadcast join comparison\n",
    "print(\"\\n Regular join (without broadcast):\")\n",
    "regular_join = large_orders_df.join(customers, \"customer_id\")\n",
    "print(f\"Regular join result: {regular_join.count()} records\")\n",
    "\n",
    "print(\"\\n Broadcast join (optimized for small tables):\")\n",
    "broadcast_join = large_orders_df.join(broadcast(customers), \"customer_id\")\n",
    "print(f\"Broadcast join result: {broadcast_join.count()} records\")\n",
    "\n",
    "# 3. Partitioning optimization\n",
    "print(\"\\n 3. Partitioning Optimization:\")\n",
    "\n",
    "# Check current partitioning\n",
    "print(f\"Orders RDD partitions: {orders.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition for better performance\n",
    "optimized_orders = orders.repartition(4, \"category\")\n",
    "print(f\"Optimized partitions: {optimized_orders.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce for reducing partitions\n",
    "coalesced_orders = orders.coalesce(2)\n",
    "print(f\"Coalesced partitions: {coalesced_orders.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 4. Query execution plan analysis\n",
    "print(\"\\n 4. Query Execution Plan Analysis:\")\n",
    "\n",
    "# Create a complex query to analyze\n",
    "complex_query = customers.alias(\"c\") \\\n",
    "    .join(orders.alias(\"o\"), \"customer_id\") \\\n",
    "    .groupBy(\"c.state\", \"o.category\") \\\n",
    "    .agg(\n",
    "        sum(\"o.amount\").alias(\"total_sales\"),\n",
    "        count(\"o.order_id\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "print(\"Complex query execution plan:\")\n",
    "complex_query.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720229c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Lake Optimization\n",
    "\n",
    "print(\"üèéÔ∏è Delta Lake Performance Optimization\\n\")\n",
    "\n",
    "# 1. OPTIMIZE command\n",
    "print(\" 1. Delta Lake OPTIMIZE:\")\n",
    "\n",
    "# Check table details before optimization\n",
    "print(\"Table details before optimization:\")\n",
    "spark.sql(f\"DESCRIBE DETAIL delta.`{orders_table_path}`\").select(\n",
    "    \"numFiles\", \"sizeInBytes\", \"minReaderVersion\", \"minWriterVersion\"\n",
    ").show()\n",
    "\n",
    "# Run OPTIMIZE to compact files\n",
    "print(\"Running OPTIMIZE command...\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{orders_table_path}`\")\n",
    "\n",
    "print(\"OPTIMIZE completed!\")\n",
    "\n",
    "# 2. Z-ORDER optimization\n",
    "print(\"\\n 2. Z-ORDER Optimization:\")\n",
    "\n",
    "# Z-ORDER by frequently queried columns\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE delta.`{orders_table_path}`\n",
    "    ZORDER BY (customer_id, category)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Z-ORDER optimization completed!\")\n",
    "print(\"Z-ORDER improves data skipping for range queries\")\n",
    "\n",
    "# 3. VACUUM for cleanup\n",
    "print(\"\\n 3. VACUUM Operations:\")\n",
    "\n",
    "# Check history before vacuum\n",
    "print(\"Delta table history:\")\n",
    "delta_orders_table = DeltaTable.forPath(spark, orders_table_path)\n",
    "delta_orders_table.history().select(\"version\", \"timestamp\", \"operation\").show(5)\n",
    "\n",
    "# Run VACUUM (note: default retention is 7 days)\n",
    "print(\"Running VACUUM (simulated - normally retains 7 days):\")\n",
    "# spark.sql(f\"VACUUM delta.`{orders_table_path}` RETAIN 0 HOURS\")  # Don't run in production!\n",
    "print(\"VACUUM removes old files - be careful with retention period!\")\n",
    "\n",
    "# 4. Statistics and data skipping\n",
    "print(\"\\n 4. Data Skipping Statistics:\")\n",
    "\n",
    "# Demonstrate data skipping with partition elimination\n",
    "filtered_query = spark.read.format(\"delta\").load(orders_table_path) \\\n",
    "    .filter(col(\"category\") == \"Electronics\")\n",
    "\n",
    "print(\"Query with data skipping (partition elimination):\")\n",
    "print(f\"Electronics orders: {filtered_query.count()}\")\n",
    "\n",
    "# Show table statistics\n",
    "print(\"\\n Delta table statistics:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED delta.`{orders_table_path}`\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21216320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slowly Changing Dimensions (SCD)\n",
    "\n",
    "def implement_scd_type2():\n",
    "    \"\"\"Implement SCD Type 2 for customer data\"\"\"\n",
    "    \n",
    "    # Create SCD table structure\n",
    "    scd_path = f\"{base_path}/customer_scd\"\n",
    "    \n",
    "    # Initial customer data with SCD fields\n",
    "    initial_customers = customers.withColumn(\"effective_date\", current_date()) \\\n",
    "        .withColumn(\"end_date\", lit(None).cast(\"date\")) \\\n",
    "        .withColumn(\"is_current\", lit(True)) \\\n",
    "        .withColumn(\"record_version\", lit(1))\n",
    "    \n",
    "    initial_customers.write.format(\"delta\").mode(\"overwrite\").save(scd_path)\n",
    "    print(\"‚úÖ Initial SCD table created\")\n",
    "    \n",
    "    # Simulate customer data changes\n",
    "    customer_updates = [\n",
    "        (2, \"Bob Smith\", \"bob.smith@newemail.com\", \"2023-02-20\", \"Premium\", \"California\"),  # Email change\n",
    "        (3, \"Carol Davis\", \"carol@email.com\", \"2023-03-10\", \"Standard\", \"Texas\")  # Tier change\n",
    "    ]\n",
    "    \n",
    "    updates_df = spark.createDataFrame(customer_updates, customers_schema)\n",
    "    \n",
    "    # Implement SCD Type 2 logic\n",
    "    scd_table = DeltaTable.forPath(spark, scd_path)\n",
    "    \n",
    "    # Step 1: Close existing records for changed customers\n",
    "    scd_table.alias(\"scd\").merge(\n",
    "        updates_df.alias(\"updates\"),\n",
    "        \"scd.customer_id = updates.customer_id AND scd.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"scd.email != updates.email OR scd.tier != updates.tier\",\n",
    "        set={\n",
    "            \"end_date\": \"current_date()\",\n",
    "            \"is_current\": \"false\"\n",
    "        }\n",
    "    ).execute()\n",
    "    \n",
    "    # Step 2: Insert new versions\n",
    "    new_versions = updates_df.withColumn(\"effective_date\", current_date()) \\\n",
    "        .withColumn(\"end_date\", lit(None).cast(\"date\")) \\\n",
    "        .withColumn(\"is_current\", lit(True)) \\\n",
    "        .withColumn(\"record_version\", lit(2))\n",
    "    \n",
    "    new_versions.write.format(\"delta\").mode(\"append\").save(scd_path)\n",
    "    \n",
    "    print(\"SCD Type 2 updates completed\")\n",
    "    \n",
    "    # Show results\n",
    "    print(\"\\n SCD Table Results:\")\n",
    "    spark.read.format(\"delta\").load(scd_path) \\\n",
    "        .orderBy(\"customer_id\", \"record_version\") \\\n",
    "        .select(\"customer_id\", \"name\", \"email\", \"tier\", \"effective_date\", \"end_date\", \"is_current\") \\\n",
    "        .show(truncate=False)\n",
    "\n",
    "implement_scd_type2()\n",
    "\n",
    "# Data Deduplication\n",
    "def handle_duplicates():\n",
    "    \"\"\"Remove duplicates from incoming data\"\"\"\n",
    "    \n",
    "    # Create data with duplicates\n",
    "    duplicate_orders = [\n",
    "        (201, 1, \"2023-06-15\", 100.0, \"Electronics\"),\n",
    "        (201, 1, \"2023-06-15\", 100.0, \"Electronics\"),  # Exact duplicate\n",
    "        (202, 2, \"2023-06-16\", 150.0, \"Books\"),\n",
    "        (202, 2, \"2023-06-16\", 155.0, \"Books\")  # Partial duplicate (different amount)\n",
    "    ]\n",
    "    \n",
    "    orders_with_dupes = spark.createDataFrame(duplicate_orders, orders_schema)\n",
    "    \n",
    "    print(f\"Orders with duplicates: {orders_with_dupes.count()} records\")\n",
    "    orders_with_dupes.show()\n",
    "    \n",
    "    # Method 1: Remove exact duplicates\n",
    "    exact_deduped = orders_with_dupes.dropDuplicates()\n",
    "    print(f\"\\n After exact deduplication: {exact_deduped.count()} records\")\n",
    "    \n",
    "    # Method 2: Remove duplicates based on key columns only\n",
    "    key_deduped = orders_with_dupes.dropDuplicates([\"order_id\", \"customer_id\", \"order_date\"])\n",
    "    print(f\"After key-based deduplication: {key_deduped.count()} records\")\n",
    "    \n",
    "    # Method 3: Advanced deduplication with window functions\n",
    "    window_spec = Window.partitionBy(\"order_id\", \"customer_id\", \"order_date\") \\\n",
    "        .orderBy(desc(\"amount\"))  # Keep record with highest amount\n",
    "    \n",
    "    advanced_deduped = orders_with_dupes \\\n",
    "        .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"row_number\") == 1) \\\n",
    "        .drop(\"row_number\")\n",
    "    \n",
    "    print(f\"After advanced deduplication: {advanced_deduped.count()} records\")\n",
    "    advanced_deduped.show()\n",
    "\n",
    "handle_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Handling and Monitoring\n",
    "def robust_etl_pipeline():\n",
    "    \"\"\"Demonstrate error handling in ETL pipeline\"\"\"\n",
    "    \n",
    "    # Create some problematic data\n",
    "    problematic_data = [\n",
    "        (301, 1, \"2023-06-20\", 200.0, \"Electronics\"),      # Good record\n",
    "        (302, None, \"2023-06-21\", 150.0, \"Books\"),         # Null customer_id\n",
    "        (303, 2, \"invalid-date\", 100.0, \"Clothing\"),       # Invalid date\n",
    "        (304, 3, \"2023-06-22\", -50.0, \"Electronics\"),      # Negative amount\n",
    "        (305, 999, \"2023-06-23\", 75.0, \"Books\")            # Non-existent customer\n",
    "    ]\n",
    "    \n",
    "    problematic_df = spark.createDataFrame(problematic_data, orders_schema)\n",
    "    \n",
    "    print(\"Processing data with potential errors...\")\n",
    "    \n",
    "    # Error handling strategy\n",
    "    valid_records = []\n",
    "    error_records = []\n",
    "    \n",
    "    def validate_record(row):\n",
    "        \"\"\"Validate individual record\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check for null customer_id\n",
    "        if row.customer_id is None:\n",
    "            errors.append(\"Null customer_id\")\n",
    "        \n",
    "        # Check for negative amounts\n",
    "        if row.amount <= 0:\n",
    "            errors.append(\"Invalid amount\")\n",
    "        \n",
    "        # Check date format (simplified)\n",
    "        try:\n",
    "            datetime.strptime(row.order_date, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            errors.append(\"Invalid date format\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    # Process each record\n",
    "    for row in problematic_df.collect():\n",
    "        is_valid, errors = validate_record(row)\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_records.append(row)\n",
    "        else:\n",
    "            error_record = {\n",
    "                \"original_data\": row.asDict(),\n",
    "                \"errors\": errors,\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"pipeline_run_id\": str(uuid.uuid4())\n",
    "            }\n",
    "            error_records.append(error_record)\n",
    "    \n",
    "    print(f\"Valid records: {len(valid_records)}\")\n",
    "    print(f\"Error records: {len(error_records)}\")\n",
    "    \n",
    "    # Create error log\n",
    "    if error_records:\n",
    "        error_df = spark.createDataFrame(error_records)\n",
    "        error_path = f\"{base_path}/error_logs\"\n",
    "        error_df.write.format(\"delta\").mode(\"append\").save(error_path)\n",
    "        \n",
    "        print(\"\\n Error Log:\")\n",
    "        error_df.select(\"original_data\", \"errors\").show(truncate=False)\n",
    "    \n",
    "    return len(valid_records), len(error_records)\n",
    "\n",
    "valid_count, error_count = robust_etl_pipeline()\n",
    "\n",
    "# Monitoring and alerting patterns\n",
    "def create_pipeline_metrics():\n",
    "    \"\"\"Create monitoring metrics for pipeline\"\"\"\n",
    "    \n",
    "    pipeline_metrics = {\n",
    "        \"pipeline_name\": \"customer_orders_etl\",\n",
    "        \"run_timestamp\": datetime.now(),\n",
    "        \"records_processed\": valid_count + error_count,\n",
    "        \"records_successful\": valid_count,\n",
    "        \"records_failed\": error_count,\n",
    "        \"success_rate\": (valid_count / (valid_count + error_count)) * 100 if (valid_count + error_count) > 0 else 0,\n",
    "        \"pipeline_status\": \"SUCCESS\" if error_count == 0 else \"PARTIAL_FAILURE\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\n Pipeline Metrics:\")\n",
    "    for key, value in pipeline_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # In production, this would be sent to monitoring system\n",
    "    # (DataDog, CloudWatch, etc.)\n",
    "    \n",
    "    return pipeline_metrics\n",
    "\n",
    "metrics = create_pipeline_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb87e9",
   "metadata": {},
   "source": [
    "## 8. SQL Analysis and Complex Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6140a3",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL\n",
    "-- Create temporary views for SQL analysis\n",
    "CREATE OR REPLACE TEMPORARY VIEW customers_view AS\n",
    "SELECT * FROM delta.`/tmp/delta-practice/customers`;\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW orders_view AS  \n",
    "SELECT * FROM delta.`/tmp/delta-practice/orders`;\n",
    "\n",
    "-- Find top 3 customers by total spending\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.tier,\n",
    "    SUM(o.amount) as total_spent,\n",
    "    COUNT(o.order_id) as total_orders,\n",
    "    ROUND(AVG(o.amount), 2) as avg_order_value\n",
    "FROM customers_view c\n",
    "JOIN orders_view o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.name, c.tier\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8b00f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Window functions - Rank orders within each customer\n",
    "SELECT \n",
    "    customer_id,\n",
    "    order_id,\n",
    "    order_date,\n",
    "    amount,\n",
    "    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as order_rank,\n",
    "    SUM(amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS UNBOUNDED PRECEDING) as running_total,\n",
    "    LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as previous_order_amount,\n",
    "    LEAD(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_amount\n",
    "FROM orders_view\n",
    "ORDER BY customer_id, order_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb21ed",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Complex analytics - Customer behavior analysis\n",
    "WITH customer_metrics AS (\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.name,\n",
    "        c.tier,\n",
    "        c.state,\n",
    "        COUNT(o.order_id) as order_count,\n",
    "        SUM(o.amount) as total_spent,\n",
    "        AVG(o.amount) as avg_order_value,\n",
    "        MIN(o.order_date) as first_order_date,\n",
    "        MAX(o.order_date) as last_order_date,\n",
    "        DATEDIFF(MAX(o.order_date), MIN(o.order_date)) as customer_lifetime_days\n",
    "    FROM customers_view c\n",
    "    LEFT JOIN orders_view o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.name, c.tier, c.state\n",
    "),\n",
    "tier_stats AS (\n",
    "    SELECT \n",
    "        tier,\n",
    "        AVG(total_spent) as tier_avg_spending,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_spent) as tier_median_spending\n",
    "    FROM customer_metrics\n",
    "    GROUP BY tier\n",
    ")\n",
    "SELECT \n",
    "    cm.*,\n",
    "    ts.tier_avg_spending,\n",
    "    ts.tier_median_spending,\n",
    "    CASE \n",
    "        WHEN cm.total_spent > ts.tier_avg_spending THEN 'Above Average'\n",
    "        WHEN cm.total_spent < ts.tier_avg_spending THEN 'Below Average'\n",
    "        ELSE 'Average'\n",
    "    END as spending_vs_tier_avg,\n",
    "    CASE \n",
    "        WHEN cm.customer_lifetime_days = 0 THEN 'New Customer'\n",
    "        WHEN cm.customer_lifetime_days <= 30 THEN 'Recent Customer' \n",
    "        WHEN cm.customer_lifetime_days <= 90 THEN 'Regular Customer'\n",
    "        ELSE 'Long-term Customer'\n",
    "    END as customer_segment\n",
    "FROM customer_metrics cm\n",
    "JOIN tier_stats ts ON cm.tier = ts.tier\n",
    "ORDER BY cm.total_spent DESC;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
